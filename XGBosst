%matplotlib inline

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

from sklearn.modelimport re
import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

from xgboost.sklearn import XGBRegressor 
from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import StratifiedKFold 
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

from sklearn.pipeline import Pipeline

regex = re.compile(r"\[|\]|<", re.IGNORECASE)

from google.colab import files
uploaded = files.upload()


df = pd.read_excel('Quasi-Yagi.xlsx')
data = df.copy()
x = data.drop(labels=['G1', 'G2', 'G3'], axis=1) 
y = data[['G3']]


def experiment_train_kfold(data, estimator, param_grid, to_drop, targets, metric='r2', normalize_target=False):
    X = data.drop(labels=to_drop, axis=1)
    Y = data[targets]
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)

    std = StandardScaler()
    X_train_std = std.fit_transform(X_train)
    X_test_std = std.transform(X_test)

    if normalize_target:
        std_ = StandardScaler()
        Y_train_final = std_.fit_transform(Y_train)
        Y_test_final = std_.transform(Y_test)
    else:
        Y_train_final = Y_train
        Y_test_final = Y_test

    n_dim = X.shape[1]
    kfold = KFold(n_splits=12)
    
    if isinstance(estimator, SVR):
        estimator = MultiOutputRegressor(estimator)
    
    rand = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid,
                        scoring=metric, cv=kfold, verbose=1, n_jobs=-1)
    rand.fit(X_train_std, Y_train_final)
    return rand.best_estimator_, std, X_test_std, Y_test_final
    
    
    def plot_target_var(var_name, var_indx, y_true, y_pred):
    if var_indx is not None:
        target_true = y_true[:, var_indx]
        target_pred = y_pred[:, var_indx]
    else:
        target_true = y_true.values.ravel()
        target_pred = y_pred.ravel()
    
    num_instances = range(len(target_true))
    idxs = np.argsort(target_true)
    plt.figure(figsize=(15, 8))
    plt.scatter(num_instances, target_true[idxs], s=40, c='b', marker='x',
                                                  alpha=0.5, linewidths=1,
                                                  edgecolors='b', label='Original Data')
    plt.scatter(num_instances, target_pred[idxs], s=40, c='w', marker='o',
                                                  alpha=0.5, linewidths=1,
                                                  edgecolors='r', label='Predicted Data')
    plt.ylabel(var_name)
    plt.xlabel('Ordered target variables')
    plt.legend()
    
    
  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)
  X_train.columns = [regex.sub("_", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]
  X_test.columns = [regex.sub("_", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]
    
model = XGBRegressor()
n_estimators = [100, 200, 300, 400,500,600,700,800,900,1000]
learning_rate = [0.0001, 0.001, 0.01, 0.1]
subsample = [0.5, 0.75, 1.0]
colsample_bytree = [0.4, 0.6, 0.8, 1.0]
max_depth = [4, 6, 8, 10]

    
param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators,subsample=subsample,colsample_bytree=colsample_bytree,max_depth=max_depth)
kfold = KFold(n_splits=15, shuffle=True, random_state=7)
grid_search = GridSearchCV(model, param_grid, scoring="neg_mean_squared_error", n_jobs=-1, cv=kfold, verbose=1)
grid_result = grid_search.fit(X_train, y_train)


final_model = grid_search.best_estimator_
#xg_reg.fit(X_train,y_train)
preds = final_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))
    

